import os
import json
import argparse
import numpy as np
from tqdm import tqdm
from magvit2.models.lfqgan import VQModel
from magvit2.config import VQConfig
from torchvision import transforms
from decord import VideoReader, cpu
import torch
from torch.utils.data import Dataset, DataLoader
import gc
import torchvision.io as io
import cv2
import json


# âœ… åŠ è½½ tokenizer æ¨¡å‹å¹¶ç§»åŠ¨åˆ°æŒ‡å®š GPU
def load_model(gpu_id):
    torch.cuda.set_device(gpu_id)
    tokenizer = VQModel(
        VQConfig(),
        ckpt_path="/iag_ad_01/ad/zhangxin11/tyz/observation-reward-model/genie/magvit2.ckpt",
    ).cuda()
    tokenizer.eval()
    print(f"âœ… æ¨¡å‹å·²åŠ è½½åˆ° GPU {gpu_id}")
    return tokenizer


# âœ… è¯»å–è§†é¢‘è·¯å¾„å¹¶æŒ‰ `item_range` é€‰æ‹©æ•°æ®
def load_data(train_dir):
    data = [os.path.join(train_dir, video_dir) for video_dir in os.listdir(train_dir)]
    return data


def output_video(
    video_frames,
    output_path="/iag_ad_01/ad/zhangxin11/tyz/observation-genie/temp/temp.mp4",
    fps=30,
    frame_width=640,
    frame_height=480,
):
    # å®šä¹‰è§†é¢‘ç¼–ç æ ¼å¼å’Œåˆ›å»ºè§†é¢‘å†™å…¥å™¨
    fourcc = cv2.VideoWriter_fourcc(*"mp4v")
    out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))

    # éå†æ¯ä¸€å¸§å¹¶å†™å…¥è§†é¢‘
    for i in range(len(video_frames)):
        frame = video_frames[i]
        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)
        # ç¡®ä¿å¸§æ˜¯ uint8 ç±»å‹
        if frame.dtype != np.uint8:
            frame = (frame * 255).astype(np.uint8)

        # å†™å…¥å¸§åˆ°è§†é¢‘
        out.write(frame)

    # é‡Šæ”¾è§†é¢‘å†™å…¥å™¨
    out.release()


# âœ… å®šä¹‰ VideoDataset ç±»
class VideoDataset(Dataset):
    def __init__(self, video_paths):
        self.video_paths = video_paths
        self.resize_transform = transforms.Resize((256, 256))

    def __len__(self):
        return len(self.video_paths)

    def __getitem__(self, idx):
        video_path = self.video_paths[idx]
        data = np.load(video_path, allow_pickle=True)["arr_0"].tolist()
        temp_path = (
            f"/iag_ad_01/ad/zhangxin11/tyz/observation-genie/temp/temp_{idx}.mp4"
        )
        output_video(data["observation"]["rgb"], output_path=temp_path)
        # åŠ è½½è§†é¢‘å¸§å¹¶è¿›è¡Œ Resize å¤„ç†
        vr = VideoReader(
            temp_path,
            ctx=cpu(0),
            num_threads=4,
        )
        frames = vr.get_batch(range(len(vr))).asnumpy()
        frames = torch.from_numpy(frames).permute(0, 3, 1, 2) / 255.0  # [N, 3, H, W]
        resized_frames = torch.stack([self.resize_transform(frame) for frame in frames])

        actions = data["action"]
        action_all = np.concatenate(
            [
                actions["end"]["position"],
                actions["end"]["orientation"],
                actions["effector"]["position_gripper"].reshape(
                    actions["effector"]["position_gripper"].shape + (1,)
                ),
            ],
            axis=2,
        )  # N, 1, 8

        return (
            resized_frames,
            action_all,
            int(os.path.basename(video_path).split(".")[0].split("_")[-1]),
        )


# âœ… è®¡ç®—å½“å‰ GPU å¤„ç†çš„æ•°æ®æ€»å¸§æ•°
def calculate_total_frames(data):
    total_frame = 0
    for item in tqdm(data):
        total_frame += (
            np.load(item, allow_pickle=True)["arr_0"]
            .tolist()["observation"]["rgb"]
            .shape[0]
        )
    print(f"âœ… è®¡ç®—æ€»å¸§æ•°: {total_frame}")
    return total_frame


# âœ… åˆ›å»º memmap æ–‡ä»¶è·¯å¾„ï¼Œæ ¹æ® `item_range` åˆ›å»ºä¸åŒçš„æ–‡ä»¶
def create_memmap_files(output_dir, total_frame):
    """
    video_data_path = f"{output_dir}/video_item{item_range[0]}_{item_range[1]}.bin"
    segment_data_path = f"{output_dir}/segment_ids_item{item_range[0]}_{item_range[1]}.bin"
    action_data_path = f"{output_dir}/action_item{item_range[0]}_{item_range[1]}.bin"
    """
    os.makedirs(output_dir, exist_ok=True)
    video_data_path = f"{output_dir}/video.bin"
    segment_data_path = f"{output_dir}/segment_ids.bin"
    action_data_path = f"{output_dir}/action.bin"
    video_data = np.memmap(
        video_data_path, dtype=np.uint32, mode="w+", shape=(total_frame, 16, 16)
    )
    segment_data = np.memmap(
        segment_data_path, dtype=np.int32, mode="w+", shape=(total_frame, 1)
    )
    action_data = np.memmap(
        action_data_path, dtype=np.float32, mode="w+", shape=(total_frame, 8)
    )
    print(
        f"âœ… åˆ›å»º memmap æ–‡ä»¶ï¼š\n  - {video_data_path}\n  - {segment_data_path}\n  - {action_data_path}\n"
    )

    # Dictionary with the specified data
    metadata = {
        "token_dtype": "uint32",
        "s": 16,
        "h": 16,
        "w": 16,
        "vocab_size": 262144,
        "hz": 30,
        "tokenizer_ckpt": "data/magvit2.ckpt",
        "num_images": total_frame,
    }

    # Path to save the JSON file
    file_path = f"{output_dir}/metadata.json"

    # Writing the dictionary to a JSON file
    with open(file_path, "w") as file:
        json.dump(metadata, file)

    return video_data, segment_data, action_data


# âœ… å¤„ç†è§†é¢‘æ•°æ®å¹¶ç¼–ç 
def process_videos(data_loader, tokenizer, video_data, segment_data, action_data):
    frame_idx = 0
    batch_size = 32

    for resized_frames, clip_actions, video_id in tqdm(data_loader):
        resized_frames = resized_frames[0].cuda(non_blocking=True)
        clip_actions = clip_actions[0].numpy()

        # åˆ†æ‰¹æ¬¡å¤„ç†å¸§ï¼Œé˜²æ­¢ GPU å†…å­˜æº¢å‡º
        num_batches = (resized_frames.size(0) + batch_size - 1) // batch_size
        for i in range(num_batches):
            start, end = i * batch_size, min(
                (i + 1) * batch_size, resized_frames.size(0)
            )
            batch_frames = resized_frames[start:end].cuda(non_blocking=True)
            batch_actions = clip_actions[start:end]
            # FP16 åŠ é€Ÿæ¨ç†
            with torch.no_grad():
                quant, _, _, _ = tokenizer.encode(batch_frames * 2 - 1)
                token_ids = (
                    tokenizer.quantize.bits_to_indices(quant.permute(0, 2, 3, 1) > 0)
                    .cpu()
                    .numpy()
                )

            # âœ… å†™å…¥ memmap æ–‡ä»¶
            video_data[frame_idx : frame_idx + batch_frames.size(0)] = token_ids
            action_data[frame_idx : frame_idx + batch_frames.size(0)] = (
                batch_actions.reshape(-1, 8)
            )
            segment_data[frame_idx : frame_idx + batch_frames.size(0)] = video_id
            # æ›´æ–°å¸§ç´¢å¼•
            frame_idx += batch_frames.size(0)
            # é‡Šæ”¾å†…å­˜
            del quant, token_ids, batch_frames, batch_actions
            torch.cuda.empty_cache()
            gc.collect()

    print(f"âœ… å¤„ç†å®Œæˆï¼Œæ€»å¸§æ•°: {frame_idx}")


# âœ… è§£æå‘½ä»¤è¡Œå‚æ•°
def parse_args():
    parser = argparse.ArgumentParser(
        description="Video Encoding with MagViT on Multiple GPUs"
    )
    parser.add_argument(
        "--source_path",
        type=str,
        default="/iag_ad_01/ad/zhangxin11/tyz/observation-reward-model/data/sample/tomato_random/data",
    )
    parser.add_argument(
        "--output_path",
        type=str,
        default="/iag_ad_01/ad/zhangxin11/tyz/observation-reward-model/data/genie_data_debug",
    )
    parser.add_argument("--gpu_id", type=int, default=0, help="GPU ID to use")
    return parser.parse_args()


# âœ… ä¸»å‡½æ•°ï¼šåè°ƒå„ä¸ªæ¨¡å—
def main():
    # è§£æå‚æ•°
    args = parse_args()
    gpu_id = args.gpu_id

    # åŠ è½½æ¨¡å‹å’Œæ•°æ®
    tokenizer = load_model(gpu_id)
    data = load_data(args.source_path)  # npz paths
    train_data = data[: int(0.8 * len(data))]
    val_data = data[int(0.8 * len(data)) :]

    # åŠ è½½ Dataset å’Œ DataLoader
    train_video_dataset = VideoDataset(train_data)
    val_video_dataset = VideoDataset(val_data)

    train_data_loader = DataLoader(
        train_video_dataset, batch_size=1, num_workers=4, pin_memory=True
    )
    val_data_loader = DataLoader(
        val_video_dataset, batch_size=1, num_workers=4, pin_memory=True
    )

    # è®¡ç®—æ€»å¸§æ•°
    train_total_frame = calculate_total_frames(train_data)
    val_total_frame = calculate_total_frames(val_data)

    # åˆ›å»º memmap æ–‡ä»¶
    output_dir = args.output_path
    os.makedirs(output_dir, exist_ok=True)
    train_video_data, train_segment_data, train_action_data = create_memmap_files(
        os.path.join(output_dir, "train"), train_total_frame
    )
    val_video_data, val_segment_data, val_action_data = create_memmap_files(
        os.path.join(output_dir, "val"), val_total_frame
    )

    # å¤„ç†è§†é¢‘æ•°æ®
    process_videos(
        train_data_loader,
        tokenizer,
        train_video_data,
        train_segment_data,
        train_action_data,
    )
    process_videos(
        val_data_loader,
        tokenizer,
        val_video_data,
        val_segment_data,
        val_action_data,
    )

    print(
        f"ğŸ‰ ä»»åŠ¡å®Œæˆï¼GPU {gpu_id} å¤„ç† {len(data)} ä¸ªè§†é¢‘ï¼Œæ•°æ®å·²ä¿å­˜åˆ° {output_dir}"
    )


# âœ… å¯åŠ¨ä¸»ç¨‹åº
if __name__ == "__main__":
    main()
