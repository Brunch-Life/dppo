import os
import json
import argparse
import numpy as np
from tqdm import tqdm
from magvit2.models.lfqgan import VQModel
from magvit2.config import VQConfig
from torchvision import transforms
import torch
from torch.utils.data import Dataset, DataLoader
import gc
import torchvision.io as io
import cv2
import json
from PIL import Image
import subprocess
from concurrent.futures import ThreadPoolExecutor, as_completed
from scipy.spatial.transform import Rotation as R


# âœ… è§£æå‘½ä»¤è¡Œå‚æ•°
def parse_args():
    parser = argparse.ArgumentParser(
        description="Video Encoding with MagViT on Multiple GPUs"
    )
    parser.add_argument(
        "--source_path",
        type=str,
        default="/manifold-obs/bingwen/Datasets/wooden/bowl/TabletopPickPlaceEnv-v1",
    )
    parser.add_argument(
        "--output_path",
        type=str,
        default="/ML-vePFS/tangyinzhou/encoded_maniskill_delta_multi_view_temp",
    )
    parser.add_argument("--gpu_id", type=int, default=0, help="GPU ID to use")
    parser.add_argument(
        "--task", type=str, default="20250907_154533", help="each part is 1w"
    )
    parser.add_argument("--sigma", type=float, default=-1, help="each part is 1w")
    parser.add_argument("--multiview", action="store_true")
    return parser.parse_args()


# âœ… åŠ è½½ tokenizer æ¨¡å‹å¹¶ç§»åŠ¨åˆ°æŒ‡å®š GPU
def load_model(gpu_id):
    torch.cuda.set_device(gpu_id)
    tokenizer = VQModel(
        VQConfig(),
        ckpt_path="/ML-vePFS/tangyinzhou/observation-genie/genie/magvit2.ckpt",
    ).cuda()
    tokenizer.eval()
    print(f"âœ… æ¨¡å‹å·²åŠ è½½åˆ° GPU {gpu_id}")
    return tokenizer


# âœ… åŠ è½½ tokenizer æ¨¡å‹å¹¶ç§»åŠ¨åˆ°æŒ‡å®š GPU
def load_text_model(gpu_id):
    from transformers import T5Tokenizer, T5Model

    torch.cuda.set_device(gpu_id)
    tokenizer = None
    model = None
    print(f"âœ… textæ¨¡å‹å·²åŠ è½½åˆ° GPU {gpu_id}")
    return tokenizer, model


# âœ… è¯»å–è§†é¢‘è·¯å¾„å¹¶æŒ‰ `item_range` é€‰æ‹©æ•°æ®
def load_data(train_dir, tid):
    data = []
    for task in os.listdir(os.path.join(train_dir, tid)):
        data.extend(
            [
                os.path.join(train_dir, tid, task, "success", x)
                for x in os.listdir(os.path.join(train_dir, tid, task, "success"))
            ]
        )
    return data


def exp_grow(N, rate=4):
    """è¿”å›é•¿åº¦ä¸º N çš„è½¯ deltaï¼Œå³°å€¼åœ¨æœ€åä¸€ä¸ªå…ƒç´ å¤„ä¸º 1"""
    k = np.arange(N)  # 0, 1, ..., n-1
    seq = np.exp(rate * (k / (N - 1) - 1))  # æœ€åä¸€ä¸ªå…ƒç´ ä¸º exp(0)=1
    return seq / max(seq)


def get_dones(pos, grip):
    dones = np.zeros((pos.shape[0],))
    t_high = (pos[:, -1].reshape(-1) * np.abs(grip - 1).reshape(-1)).argmax()
    pick = dones[:t_high]
    place = dones[t_high:]
    pick_delta = exp_grow(len(pick))
    place_delta = exp_grow(len(place))
    dones = np.concatenate([pick_delta, place_delta + 10])
    return dones


def get_pose_from_rot_pos(mat: np.ndarray, pos: np.ndarray):
    return np.concatenate(
        [
            np.concatenate([mat, pos.reshape(3, 1)], axis=-1),
            np.array([0.0, 0.0, 0.0, 1.0]).reshape(1, 4),
        ],
        axis=0,
    )


def quat_to_matrix(quaternions):
    """å°†å››å…ƒæ•°è½¬æ¢ä¸ºæ—‹è½¬çŸ©é˜µ (w, x, y, z) æˆ– (x, y, z, w)"""
    batch_size = quaternions.shape[0]
    rot_mats = np.zeros((batch_size, 3, 3))
    for i in range(batch_size):
        # å‡è®¾å››å…ƒæ•°æ ¼å¼ä¸º (x, y, z, w)ï¼Œå¦‚æœæ˜¯ (w, x, y, z) è¯·ä¿®æ”¹
        rot = R.from_quat(quaternions[i])
        rot_mats[i] = rot.as_matrix()
    return rot_mats


def get_44(state):
    """ä»çŠ¶æ€ä¸­è·å–4x4å˜æ¢çŸ©é˜µ"""
    rot = R.from_quat(state[3:7])
    pos = state[:3]
    return get_pose_from_rot_pos(rot.as_matrix(), pos)  # (4, 4)


def eight_dim_to_ten_dim_delta(state_all, action_all, chunk_size=60):
    """
    å°†8ç»´çŠ¶æ€å’Œ8ç»´åŠ¨ä½œè½¬æ¢ä¸º10ç»´delta action

    å‚æ•°:
        current_state: 8ç»´çŠ¶æ€ [B, 8]
            ç»“æ„: [x, y, z, qx, qy, qz, qw, gripper]
        target_action: 8ç»´ç›®æ ‡åŠ¨ä½œ [B, 8]
            ç»“æ„: [x, y, z, qx, qy, qz, qw, gripper]

    è¿”å›:
        delta_action: 10ç»´delta action [B, 10]
    """
    length = state_all.shape[0]
    delta_actions = []
    for i in range(length):
        state_at_obs = state_all[i]
        end = min(i + chunk_size, length)
        current_action = action_all[i:end][:, :7]
        action_gripper = action_all[i:end][:, 7:8]
        delta_action_44 = []
        state_44 = get_44(state_at_obs)
        for j in range(end - i):
            action_44 = get_44(current_action[j])
            delta_44 = np.linalg.inv(state_44) @ action_44
            delta_action_44.append(delta_44)  # list of (4,4)
        delta_action_44 = np.stack(delta_action_44)  # (60, 4, 4)
        mat_6 = delta_action_44[:, :3, :2].reshape(delta_action_44.shape[0], 6)
        delta_pos = delta_action_44[:, :3, 3].reshape(delta_action_44.shape[0], 3)
        delta_action_10 = np.concatenate(
            [
                delta_pos,
                mat_6,
                action_gripper,
            ],
            axis=-1,
        )
        # å¦‚æœä¸å¤Ÿchunk_sizeï¼Œåˆ™ç”¨-10è¡¥é½
        if delta_action_10.shape[0] < chunk_size:
            delta_action_10 = np.pad(
                delta_action_10,
                ((0, chunk_size - delta_action_10.shape[0]), (0, 0)),
                "constant",
                constant_values=-10,
            )
        delta_actions.append(delta_action_10)
    delta_actions = np.stack(delta_actions, axis=0)  # (B, 60, 10)
    return delta_actions


# âœ… å®šä¹‰ VideoDataset ç±»
class VideoDataset(Dataset):
    def __init__(self, video_paths, sigma, multiview=False):
        self.video_paths = video_paths
        self.resize_transform = transforms.Resize((256, 256))
        self.sigma = sigma
        self.multiview = multiview

    def __len__(self):
        return len(self.video_paths)

    def __getitem__(self, idx):
        video_path = self.video_paths[idx]
        data = np.load(video_path, allow_pickle=True)["arr_0"].tolist()
        actions = data["action"]
        states = data["state"]

        action_all = np.concatenate(
            [
                actions["end"]["position"],  # (..., 3)
                actions["end"]["orientation"],  # (..., 4)
                actions["effector"]["position_gripper"].reshape(
                    actions["effector"]["position_gripper"].shape + (1,)
                ),
            ],
            axis=-1,
        ).reshape(
            -1, 8
        )  # (n_steps, 8)
        state_all = np.concatenate(
            [
                states["end"]["position"],  # (..., 3)
                states["end"]["orientation"],  # (..., 4)
                states["effector"]["position_gripper"].reshape(
                    states["effector"]["position_gripper"].shape + (1,)
                ),
            ],
            axis=-1,
        ).reshape(
            -1, 8
        )  # (n_steps, 8)
        delta_actions = eight_dim_to_ten_dim_delta(
            state_all=state_all, action_all=action_all
        )
        return (
            delta_actions,
            idx,
        )


def get_frame_count(video_path):
    """è·å–å•ä¸ªè§†é¢‘çš„å¸§æ•°"""
    video = cv2.VideoCapture(video_path)
    if not video.isOpened():
        raise ValueError(f"æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶: {video_path}")
    frame_count = int(video.get(cv2.CAP_PROP_FRAME_COUNT))
    video.release()
    return frame_count


# âœ… è®¡ç®—å½“å‰ GPU å¤„ç†çš„æ•°æ®æ€»å¸§æ•°
def calculate_total_frames(data, max_workers=8):
    video_paths = [
        item.replace(".npz", ".mp4").replace("/success/", "/videos/") for item in data
    ]
    total_frame = 0
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        future_to_path = {
            executor.submit(get_frame_count, path): path for path in video_paths
        }
        for future in tqdm(
            as_completed(future_to_path), total=len(video_paths), desc="ç»Ÿè®¡å¸§æ•°"
        ):
            total_frame += future.result() - 1

    print(f"âœ… è®¡ç®—æ€»å¸§æ•°: {total_frame}")
    return total_frame


# âœ… åˆ›å»º memmap æ–‡ä»¶è·¯å¾„ï¼Œæ ¹æ® `item_range` åˆ›å»ºä¸åŒçš„æ–‡ä»¶
def create_memmap_files(output_dir, total_frame, is_multiview=False):
    """
    video_data_path = f"{output_dir}/video_item{item_range[0]}_{item_range[1]}.bin"
    segment_data_path = f"{output_dir}/segment_ids_item{item_range[0]}_{item_range[1]}.bin"
    action_data_path = f"{output_dir}/action_item{item_range[0]}_{item_range[1]}.bin"
    """
    os.makedirs(output_dir, exist_ok=True)
    action_data_path = f"{output_dir}/delta_action_10dim.bin"
    action_data = np.memmap(
        action_data_path, dtype=np.float32, mode="w+", shape=(total_frame, 60, 10)
    )

    print(f"âœ… åˆ›å»º memmap æ–‡ä»¶ï¼š\n {action_data_path}")
    return action_data


def get_text_embedding(text_tokenizer, text_model, clip_text):
    # ä½¿ç”¨åˆ†è¯å™¨å¯¹æ–‡æœ¬è¿›è¡Œç¼–ç 
    inputs = text_tokenizer.encode_plus(
        clip_text,
        add_special_tokens=True,  # æ·»åŠ ç‰¹æ®Šæ ‡è®°
        max_length=512,  # è®¾ç½®æœ€å¤§é•¿åº¦
        padding="max_length",  # å¡«å……åˆ°æœ€å¤§é•¿åº¦
        truncation=True,  # æˆªæ–­è¿‡é•¿çš„æ–‡æœ¬
        return_attention_mask=True,  # è¿”å›æ³¨æ„åŠ›æ©ç 
        return_tensors="pt",  # è¿”å›PyTorchå¼ é‡
    )

    # è·å–è¾“å…¥IDå’Œæ³¨æ„åŠ›æ©ç 
    input_ids = inputs["input_ids"]
    attention_mask = inputs["attention_mask"]

    # ä½¿ç”¨æ¨¡å‹çš„ç¼–ç å™¨éƒ¨åˆ†è¿›è¡Œç¼–ç 
    with torch.no_grad():  # ç¦ç”¨æ¢¯åº¦è®¡ç®—ï¼ŒèŠ‚çœå†…å­˜å’Œè®¡ç®—èµ„æº
        outputs = text_model.encoder(input_ids=input_ids, attention_mask=attention_mask)

    # è·å–æœ€åä¸€å±‚çš„éšè—çŠ¶æ€
    last_hidden_state = outputs.last_hidden_state

    text_embedding = last_hidden_state[:, 0, :].cpu().numpy()[0]
    return text_embedding


# âœ… å¤„ç†è§†é¢‘æ•°æ®å¹¶ç¼–ç 
def process_videos(
    data_loader,
    tokenizer,
    text_tokenizer,
    text_model,
    action_data,
    output_dir,
):
    frame_idx = 0
    batch_size = 128

    for index, (clip_actions, video_id) in enumerate(data_loader):
        print(f"{index}/{len(data_loader)}")
        clip_actions = clip_actions[0].numpy()
        num_batches = (clip_actions.shape[0] + batch_size - 1) // batch_size
        for i in range(num_batches):
            start, end = i * batch_size, min(
                (i + 1) * batch_size, clip_actions.shape[0]
            )
            batch_actions = clip_actions[start:end]
            action_data[frame_idx : frame_idx + batch_actions.shape[0]] = (
                batch_actions.reshape(-1, 60, 10)
            )
            frame_idx += batch_actions.shape[0]
    print(f"âœ… å¤„ç†å®Œæˆï¼Œæ€»å¸§æ•°: {frame_idx}")


# âœ… ä¸»å‡½æ•°ï¼šåè°ƒå„ä¸ªæ¨¡å—
def main():
    # è§£æå‚æ•°
    args = parse_args()
    args.multiview = True
    gpu_id = args.gpu_id

    # åŠ è½½æ¨¡å‹å’Œæ•°æ®
    tokenizer = load_model(gpu_id)
    text_tokenizer, text_encoder = load_text_model(gpu_id)
    data = load_data(args.source_path, args.task)
    train_data = data[: int(0.8 * len(data))]
    val_data = data[int(0.8 * len(data)) :]

    # åŠ è½½ Dataset å’Œ DataLoader
    train_video_dataset = VideoDataset(train_data, args.sigma, args.multiview)
    val_video_dataset = VideoDataset(val_data, args.sigma, args.multiview)

    train_data_loader = DataLoader(
        train_video_dataset, batch_size=1, num_workers=4, pin_memory=True
    )
    val_data_loader = DataLoader(
        val_video_dataset, batch_size=1, num_workers=4, pin_memory=True
    )

    # è®¡ç®—æ€»å¸§æ•°
    train_total_frame = calculate_total_frames(train_data)
    val_total_frame = calculate_total_frames(val_data)

    # åˆ›å»º memmap æ–‡ä»¶
    output_dir = args.output_path
    os.makedirs(output_dir, exist_ok=True)
    train_action_data = create_memmap_files(
        os.path.join(output_dir, f"{args.task}", "train"),
        train_total_frame,
        args.multiview,
    )

    val_action_data = create_memmap_files(
        os.path.join(output_dir, f"{args.task}", "val"),
        val_total_frame,
        args.multiview,
    )

    # å¤„ç†è§†é¢‘æ•°æ®
    process_videos(
        train_data_loader,
        tokenizer,
        text_tokenizer,
        text_encoder,
        train_action_data,
        os.path.join(output_dir, f"{args.task}", "train"),
    )
    process_videos(
        val_data_loader,
        tokenizer,
        text_tokenizer,
        text_encoder,
        val_action_data,
        os.path.join(output_dir, f"{args.task}", "val"),
    )

    print(
        f"ğŸ‰ ä»»åŠ¡å®Œæˆï¼GPU {gpu_id} å¤„ç† {len(data)} ä¸ªè§†é¢‘ï¼Œæ•°æ®å·²ä¿å­˜åˆ° {output_dir}"
    )


# âœ… å¯åŠ¨ä¸»ç¨‹åº
if __name__ == "__main__":
    main()
